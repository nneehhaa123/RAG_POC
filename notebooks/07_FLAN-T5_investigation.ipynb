{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b890d1c-1dd9-468f-bbf9-aee8a83989fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "\n",
    "\n",
    "from dpp_helpline_qa.model_validation.model_validation import cal_em_score, calculate_semantic_similarity\n",
    "from dpp_helpline_qa.modelling.question_answer import load_model_flan, answer_question_flan\n",
    "from dpp_helpline_qa.modelling.semantic_search import load_model_ss, context_ranking\n",
    "from dpp_helpline_qa.preprocessing.preprocessing import process_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad94f01-d61e-41aa-b3a8-832564ddabbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# list of files to search\n",
    "files = [\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 5\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 6\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 7\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"FAQs 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 4\" + \".pdf\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ccb7251-6f24-47ad-aa52-6f4e66b254b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the model and tokenizer for semantic search\n",
    "model_semantic = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "model_ss, tokenizer_ss = load_model_ss(model_semantic)\n",
    "max_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad722edd-3e50-4640-907e-b91b1f87a3c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load and pre-process the documents to prepare for searching\n",
    "import time\n",
    "st = time.time()\n",
    "para_dfs = process_docs(files, model_ss, tokenizer_ss, max_length, 'FlatL2') #'Cosine'\n",
    "para_dfs[0].head()\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1a1e38f-3718-452f-ae43-c7b79870f713",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# identify the pre-processed files for searching\n",
    "op_files = glob.glob('../output/*/*.*')\n",
    "op_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fc53994-79fb-4d42-804d-2ec39aa07277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0e9955-7057-4946-8a0c-3d11d034e035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the model and tokenizer for question and answering\n",
    "\n",
    "model_checkpoint = \"google/flan-t5-xl\"\n",
    "model_qa, tokenizer_qa = load_model_flan(model_checkpoint, use_gpu)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcefb053-dbfd-45ab-b75d-34c436cd5e79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c680cdc-8791-4eb7-9ff7-88a31e89e815",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "answers = pd.read_excel('LLM_QA.xlsx')\n",
    "\n",
    "num_questions = len(answers)\n",
    "num_trials = 2\n",
    "\n",
    "all_final_ans = []\n",
    "\n",
    "for i in range(num_trials):\n",
    "    context = []\n",
    "    final_ans = []\n",
    "    for i in range(num_questions):\n",
    "        question = answers['Question'][i]\n",
    "        topic = answers['Primary Topic'][i]\n",
    "        actual_ans = answers['Answer'][i]\n",
    "        op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "        context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FlatL2')\n",
    "        # answer generated from top 5 contexts\n",
    "        main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "        # answer generated from only 1st context\n",
    "        ans_context = context_df['content'].values[0]\n",
    "        context.append(main_context)\n",
    "        # QA\n",
    "        output = answer_question_flan(model_qa, tokenizer_qa, ans_context, question, use_gpu)\n",
    "        final_ans.append(output)\n",
    "    all_final_ans.append(final_ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c11a22-1ed8-4f0d-afd1-0acb83b3b2c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_questions):\n",
    "    unique_answers = list(set([final_ans[i] for final_ans in all_final_ans]))\n",
    "    if len(unique_answers) > 1:\n",
    "        print(f\"{len(unique_answers)} unique answers for question {i}, the model is not consistent\")\n",
    "    else:\n",
    "        print(f\"The model is consistent for question {i}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddb7bf27-528b-4981-bb1a-62b80d1fbf0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test whether the QA model can accomodate input text longer than 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a64f887f-738f-4d93-96cd-06cf8c8dd44e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EM_score_ans_all = []\n",
    "Sbert_score_ans_all = []\n",
    "NLP_score_ans_all = []\n",
    "\n",
    "for j in range(5):\n",
    "    answers = pd.read_excel('LLM_QA.xlsx')\n",
    "    context = []\n",
    "    final_ans = []\n",
    "    EM_score_ans = []\n",
    "    Sbert_score_ans = []\n",
    "    NLP_score_ans = []\n",
    "    EM_score_context = []\n",
    "    Sbert_score_context = []\n",
    "    NLP_score_context = []\n",
    "    model_val = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "    for i in range(num_questions):\n",
    "        question = answers['Question'][i]\n",
    "        topic = answers['Primary Topic'][i]\n",
    "        actual_ans = answers['Answer'][i]\n",
    "        op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "        context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FlatL2')\n",
    "        # answer generated from top 5 contexts\n",
    "        main_context = '\\n'.join(context_df['content'].values[0:j+1])\n",
    "        # answer generated from only 1st context\n",
    "        ans_context = context_df['content'].values[0]\n",
    "        context.append(main_context)\n",
    "        # QA\n",
    "        output = answer_question_flan(model_qa, tokenizer_qa, main_context, question, use_gpu)\n",
    "        final_ans.append(output)\n",
    "        # output scoring\n",
    "        EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "        sim_score_ans = calculate_semantic_similarity(model_ss, tokenizer_ss, output, actual_ans) #model_val\n",
    "        Sbert_score_ans.append(sim_score_ans[1])\n",
    "        NLP_score_ans.append(sim_score_ans[2])\n",
    "        # context scoring\n",
    "        EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "        sim_score_cnxt = calculate_semantic_similarity(model_ss, tokenizer_ss, main_context, actual_ans) #model_val\n",
    "        Sbert_score_context.append(sim_score_cnxt[1])\n",
    "        NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "    answers['Extracted context'] = context\n",
    "    answers['Final answer'] = final_ans\n",
    "    answers['EM_Score_ans'] = EM_score_ans\n",
    "    answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "    answers['NLP_score_ans'] = NLP_score_ans\n",
    "    answers['EM_Score_context'] = EM_score_context\n",
    "    answers['Sbert_score_context'] = Sbert_score_context\n",
    "    answers['NLP_score_context'] = NLP_score_context\n",
    "    answers['context_top5'] = context_5 # remove incase only one ans from all top 5 context \n",
    "    answers.to_csv(f'ques_score_{j+1}.csv', index=False)\n",
    "\n",
    "    EM_score_ans_all.append(EM_score_ans)\n",
    "    Sbert_score_ans_all.append(Sbert_score_ans)\n",
    "    NLP_score_ans_all.append(NLP_score_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd82ba0-f148-4635-ba16-c2ac107351ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, scores in enumerate(EM_score_ans_all):\n",
    "    print(f\"Context {i+1}: {sum(scores)/len(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8e8ec48-57f8-47ba-b9c0-364f6d71bda8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, scores in enumerate(Sbert_score_ans_all):\n",
    "    print(f\"Context {i+1}: {sum(scores)/len(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b1daec7-e984-4bf3-b559-ca4c1366b26e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, scores in enumerate(NLP_score_ans_all):\n",
    "    print(f\"Context {i+1}: {sum(scores)/len(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_FLAN-T5_investigation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
