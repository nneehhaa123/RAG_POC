{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1a1781-19d7-4eaa-ad68-663bc0148dff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "\n",
    "\n",
    "from dpp_helpline_qa.model_validation.model_validation import cal_em_score, calculate_semantic_similarity\n",
    "from dpp_helpline_qa.modelling.question_answer import load_model_flan, answer_question_flan\n",
    "from dpp_helpline_qa.modelling.semantic_search import load_model_ss, context_ranking\n",
    "from dpp_helpline_qa.preprocessing.preprocessing import process_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b525b5ff-57d2-4e9a-bfc9-bf2654df95ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# list of files to search\n",
    "files = [\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 5\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 6\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 7\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"FAQs 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"ISA_Audit Standard-(UK)-230\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"KAEG-I [UK VERSION 2022]_ ISA (UK) 230 Audit documentation\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"UK_AU_AudFAQ_AD\" + \".pdf\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936bfc87-4d47-4ab0-b0d0-029fc3e733e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the model and tokenizer for semantic search\n",
    "model_semantic = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "model_ss, tokenizer_ss = load_model_ss(model_semantic)\n",
    "max_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f8ad4-fbb6-4c73-9c43-7036a4e6ac8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load and pre-process the documents to prepare for searching\n",
    "import time\n",
    "st = time.time()\n",
    "para_dfs = process_docs(files, model_ss, tokenizer_ss, max_length, 'FlatL2') #'Cosine'\n",
    "para_dfs[0].head()\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23eebf9-2f45-468c-ab45-0c28047a7b5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# identify the pre-processed files for searching\n",
    "op_files = glob.glob('../output/*/*.*')\n",
    "op_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a434eec-945c-465f-862c-a114981a2540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the model and tokenizer for question and answering\n",
    "\n",
    "# model_qa, tokenizer_qa = load_model_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a368584-f1ae-4fbd-a206-96c0f8f52a7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # saved model in dbfs\n",
    "# model_qa.save_pretrained('/dbfs/FileStore/tables/gpt4all-j')\n",
    "# tokenizer_qa.save_pretrained('/dbfs/FileStore/tables/gpt4all-j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7390257d-cdd1-42a0-8c8c-1b3ce805807e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # load the model and tokenizer for question and answering\n",
    "\n",
    "# model_checkpoint = 'google/flan-t5-xxl' #\"/dbfs/FileStore/tables/google_flan_model\" #/dbfs/FileStore/tables/google_flan_model\n",
    "# model_qa, tokenizer_qa = load_model_flan(model_checkpoint, False)\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model_checkpoint = \"nomic-ai/gpt4all-j\"\n",
    "# model_qa = AutoModelForCausalLM.from_pretrained(model_checkpoint, revision=\"v1.2-jazzy\")\n",
    "# tokenizer_qa = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3e54c9-58cd-447f-8128-c2fc0c190c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "context = '''My test counts have identified differences between the physical number of items in stock and the number recorded in the accounts. Does this mean that I can't place any reliance on the stock count as a control? No. If operating effectively, the client's stock count will detect and correct any errors in the stock balance recorded in the financial statements. Therefore a deviation is only identified in the control where KPMG's count differs from the client's final count and therefore an error has gone unnoticed.\n",
    "What else do we think about if our count results differ from inventory records? Management cannot use the results of our count to alter their inventory records as this would constitute us preparing management's financial statements. If differences are identified, management considers whether they need to investigate and ultimately quantify and record their own correction of any misstatement - which may or may not include management performing a complete physical count.\n",
    "What if there are differences between the inventory count results and management's final adjusted inventory listing? If management's subsequent investigation has determined inventory records (i.e. the final adjusted inventory listing) are correct and an adjustment is not needed, then we obtain evidence that this is appropriate and that there is no misstatement. If management's investigation identifies that an adjustment is necessary to the inventory records, then this is an audit misstatement and a potential control deviation.\n",
    "What if there are differences between the inventory count results and management's final adjusted inventory listing? If management's subsequent investigation has determined inventory records (i.e. the final adjusted inventory listing) are correct and an adjustment is not needed, then we obtain evidence that this is appropriate and that there is no control deviation. If management's investigation identifies that an adjustment is necessary to the inventory records, then this is an audit misstatement and a control deviation.\n",
    "counts? If there is a difference between the count sheets and management's final adjusted inventory listing, we investigate the reason for such difference. If our subsequent investigation determines that the final adjusted inventory listing is incorrect, then this is a control deviation. When the adjustments from book to physical are trivial, it supports the assessment that cycle counts produce results substantially the same as those which would be obtained by a complete physical count. If more than trivial adjustments are made to adjust inventory quantities as a result of the cycle counts, this challenges the effectiveness of the related transaction level controls and undermines the assessment that the same results are obtained from the perpetual inventory system as those by annual physical count. In such cases, we reassess control risk, and determine whether it is necessary for management to perform a complete physical count at or near period end that we would then attend. If the entity has remediated the cycle count controls, we may re-test the cycle counts subsequent to remediation of identified deficiencies.\n",
    "'''\n",
    "question = '''My test counts have identified differences between the physical number of items in stock and the number recorded in the accounts. Does this mean that I can't place any reliance on the stock count as a control?'''\n",
    "prompt = f\"Answer the question as truthfully as possible using and only using the provided context and if the answer is not contained within the context/text, say Irrelevant, Context: {context} \\n Question: {question} \\n Answer: \"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb126c1c-42c8-4ea3-85bb-2c6468679a79",
     "showTitle": true,
     "title": "Dolly Model"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "baseModel = \"databricks/dolly-v2-12b\"\n",
    "load_8bit = True  \n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\")\n",
    "model_qa = AutoModelForCausalLM.from_pretrained(baseModel, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "generator = pipeline(task='text-generation', model=model_qa, tokenizer=tokenizer_qa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ded9b1a-5040-4918-847f-92672ee4ede7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "import time\n",
    "st = time.time()\n",
    "answers = pd.read_excel('LLM_QA.xlsx')\n",
    "context = []\n",
    "final_ans = []\n",
    "EM_score_ans = []\n",
    "Sbert_score_ans = []\n",
    "NLP_score_ans = []\n",
    "EM_score_context = []\n",
    "Sbert_score_context = []\n",
    "NLP_score_context = []\n",
    "model_val = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "no_ques = answers.shape[0]\n",
    "for i in range(no_ques):\n",
    "    question = answers['Question'][i]\n",
    "    topic = answers['Primary Topic'][i]\n",
    "    actual_ans = answers['Answer'][i]\n",
    "    op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "    context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FaltL2')\n",
    "    # answer generated from top 5 contexts\n",
    "    main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "    # answer generated from only 1st context\n",
    "    # ans_context = context_df['content'].values[0]\n",
    "    context.append(main_context)\n",
    "    # QA\n",
    "    # output = answer_question_flan(model_qa, tokenizer_qa, main_context, question, False)\n",
    "    #output_ans = answer_question(model_qa, tokenizer_qa, main_context, question)\n",
    "    # ans = output_ans.split('Answer:')[1]\n",
    "    # output = ans.split('Question:')[0]\n",
    "    # Dolly model\n",
    "    prompt = f\"Answer the question as truthfully as possible using and only using the provided context and if the answer is not contained within the context/text, say Irrelevant, Context: {main_context} \\n Question: {question} \\n Answer: \"\n",
    "    response = generator(prompt, do_sample=True, \n",
    "                   max_new_tokens=200, temperature=0.1\n",
    "                   # min_length=250, max_length=500, \n",
    "                   #clean_up_tokenization_spaces=True,return_full_text=True\n",
    "                      )\n",
    "    output = response[0]['generated_text'].split('Answer:')[1]\n",
    "    final_ans.append(output)\n",
    "    # output scoring\n",
    "    EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "    sim_score_ans = calculate_semantic_similarity(model_ss, tokenizer_ss, output, actual_ans) #model_val\n",
    "    Sbert_score_ans.append(sim_score_ans[1])\n",
    "    NLP_score_ans.append(sim_score_ans[2])\n",
    "    # context scoring\n",
    "    EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "    sim_score_cnxt = calculate_semantic_similarity(model_ss, tokenizer_ss, main_context, actual_ans) #model_val\n",
    "    Sbert_score_context.append(sim_score_cnxt[1])\n",
    "    NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5907d7f-dbf3-4b81-98d7-40137fdb17e7",
     "showTitle": true,
     "title": "Lang chain model use"
    }
   },
   "outputs": [],
   "source": [
    "## langchain use\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "generate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "\n",
    "# automatic evaluation process\n",
    "import time\n",
    "st = time.time()\n",
    "answers = pd.read_excel('LLM_QA.xlsx')\n",
    "context = []\n",
    "final_ans = []\n",
    "EM_score_ans = []\n",
    "Sbert_score_ans = []\n",
    "NLP_score_ans = []\n",
    "EM_score_context = []\n",
    "Sbert_score_context = []\n",
    "NLP_score_context = []\n",
    "model_val = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "no_ques = answers.shape[0]\n",
    "for i in range(1): #no_ques\n",
    "    question = answers['Question'][i]\n",
    "    topic = answers['Primary Topic'][i]\n",
    "    actual_ans = answers['Answer'][i]\n",
    "    op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "    context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FaltL2')\n",
    "    # answer generated from top 5 contexts\n",
    "    main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "    # answer generated from only 1st context\n",
    "    # ans_context = context_df['content'].values[0]\n",
    "    context.append(main_context)\n",
    "    # QA\n",
    "    # Dolly model with langchain\n",
    "    #template for an instruction with input\n",
    "    prompt_with_context = PromptTemplate(\n",
    "        input_variables=[\"instruction\", \"context\"],\n",
    "        template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "    \n",
    "    hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "    llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "    print(llm_context_chain.predict(instruction=question, context=main_context).lstrip())\n",
    "\n",
    "    # output = response[0]['generated_text'].split('Answer:')[1]\n",
    "    # final_ans.append(output)\n",
    "    # # output scoring\n",
    "    # EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "    # sim_score_ans = calculate_semantic_similarity(model_ss, tokenizer_ss, output, actual_ans) #model_val\n",
    "    # Sbert_score_ans.append(sim_score_ans[1])\n",
    "    # NLP_score_ans.append(sim_score_ans[2])\n",
    "    # # context scoring\n",
    "    # EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "    # sim_score_cnxt = calculate_semantic_similarity(model_ss, tokenizer_ss, main_context, actual_ans) #model_val\n",
    "    # Sbert_score_context.append(sim_score_cnxt[1])\n",
    "    # NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef7b19d-049c-4e1e-bb66-65338e350458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "answers['Extracted context'] = context\n",
    "answers['Final answer'] = final_ans\n",
    "answers['EM_Score_ans'] = EM_score_ans\n",
    "answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "answers['NLP_score_ans'] = NLP_score_ans\n",
    "answers['EM_Score_context'] = EM_score_context\n",
    "answers['Sbert_score_context'] = Sbert_score_context\n",
    "answers['NLP_score_context'] = NLP_score_context\n",
    "#answers['context_top5'] = context_5 # remove incase only one ans from all top 5 context \n",
    "answers.to_csv('ques_score.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb9755d0-6e65-4659-92b1-5be02207d16d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate 5 answers from top 5 context and select best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "346d326f-3c8a-4540-b917-206c5e0c2524",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "import time\n",
    "st = time.time()\n",
    "answers = pd.read_excel('LLM_QA.xlsx')\n",
    "context = []\n",
    "final_ans = []\n",
    "EM_score_ans = []\n",
    "Sbert_score_ans = []\n",
    "NLP_score_ans = []\n",
    "EM_score_context = []\n",
    "Sbert_score_context = []\n",
    "NLP_score_context = []\n",
    "context_5 = []\n",
    "for i in range(8):\n",
    "    question = answers['Question'][i]\n",
    "    topic = answers['Primary Topic'][i]\n",
    "    actual_ans = answers['Answer'][i]\n",
    "\n",
    "    op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "    context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FlatL2')\n",
    "    main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "    context_5.append(main_context)\n",
    "    con_score = []\n",
    "    op_list = []\n",
    "    for j in range(5):\n",
    "        con = context_df['content'].values[j]\n",
    "        if GPU:\n",
    "            input_ids = tokenizer(f\"{con_score}\\n\\n{question}\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        else:\n",
    "            input_ids = tokenizer(f\"{con}\\n\\n{question}\", return_tensors=\"pt\").input_ids\n",
    "        temp_op = tokenizer.decode(model.generate(input_ids, max_length=200, min_length=50)[0])\n",
    "        temp_em_scr = cal_em_score(temp_op, actual_ans)\n",
    "        temp_sim_scr = calculate_semantic_similarity(temp_op, actual_ans)\n",
    "        temp_scr = (temp_em_scr+temp_sim_scr[1])/2\n",
    "        con_score.append(temp_scr)\n",
    "        op_list.append(temp_op)\n",
    "    best_ind = max(enumerate(con_score),key=lambda x: x[1])[0]\n",
    "    context.append(context_df['content'].values[best_ind])\n",
    "    output = op_list[best_ind]\n",
    "    final_ans.append(output)\n",
    "    # output scoring\n",
    "    EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "    sim_score_ans = calculate_semantic_similarity(output, actual_ans)\n",
    "    Sbert_score_ans.append(sim_score_ans[1])\n",
    "    NLP_score_ans.append(sim_score_ans[2])\n",
    "    # context scoring\n",
    "    EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "    sim_score_cnxt = calculate_semantic_similarity(main_context, actual_ans)\n",
    "    Sbert_score_context.append(sim_score_cnxt[1])\n",
    "    NLP_score_context.append(sim_score_cnxt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d8d349-4a12-443c-bbeb-fb3547858264",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9f98fe0-075c-4eea-bd16-1374bc31278f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generating multiple answers from top5 contexts also gives similar performance only, however, time is 5 times [60min] as for each ans now 5 times ans are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfb5842-b49f-4e78-9031-657f4cf48328",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in answers.columns[9:]:\n",
    "    print(f\"{col}_mean: {round(answers[col].mean(), 2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dd29f38-472b-4f09-b079-a1521ba08934",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Longformer trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3b1ba45-d3b3-46e4-9760-6af0fb8a130f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "m1 = \"allenai/longformer-base-4096\" #\"navteca/multi-qa-mpnet-base-cos-v1\"\n",
    "model = LongformerModel.from_pretrained(m1)\n",
    "tokenizer = LongformerTokenizer.from_pretrained(m1)\n",
    "from dpp_helpline_qa.preprocessing.preprocessing import get_embeddings\n",
    "\n",
    "text_list = ['this is good', 'this worse']\n",
    "\n",
    "emb = get_embeddings(text_list, model, tokenizer).cpu().detach().numpy()\n",
    "cosine_similarity(emb[[0]], emb[[1]]) # 0.99  where as with mpnet model its coming as 0.27 which makes sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bec52e7-73d2-4c70-84b2-c1c1242b6a72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "sentence embedding doesnt make sense so cant be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6079629-290b-48c0-9e9e-052b85802a21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "global_attention_mask = [1].extend([0]*encoding[\"input_ids\"].shape[-1])\n",
    "\n",
    "encoding[\"global_attention_mask\"] = global_attention_mask\n",
    "\n",
    "# I don't want to use it for Question-Answer use-case. I just need the sentence embeddings\n",
    "# default is local attention everywhere\n",
    "# the forward method will automatically set global attention on question tokens\n",
    "\n",
    "o = model(**encoding)\n",
    "\n",
    "sentence_embedding = o.last_hidden_state[:,0]\n",
    "n1 = sentence_embedding.cpu().detach().numpy()\n",
    "cosine_similarity(n1[[0]], n1[[1]]) # 0.99 this method also gives similar results as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5659a5da-b351-4ec0-bf5a-88c4553ddb8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_QA_Model_iteration",
   "widgets": {}
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
