{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6aee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "\n",
    "from dpp_helpline_qa.model_validation.model_validation import cal_em_score, calculate_semantic_similarity\n",
    "\n",
    "pd.set_option('display.max_colwidth', 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_documents = SimpleDirectoryReader('data/inventory').load_data()\n",
    "print(len(in_documents))\n",
    "mat_documents = SimpleDirectoryReader('data/materiality').load_data()\n",
    "print(len(mat_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_documents[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8da003",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = GPTSimpleVectorIndex.from_documents(in_documents)\n",
    "mat_index = GPTSimpleVectorIndex.from_documents(mat_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "import time\n",
    "st = time.time()\n",
    "answers = pd.read_excel('LLM_QA.xlsx')\n",
    "context = []\n",
    "final_ans = []\n",
    "EM_score_ans = []\n",
    "Sbert_score_ans = []\n",
    "NLP_score_ans = []\n",
    "EM_score_context = []\n",
    "Sbert_score_context = []\n",
    "NLP_score_context = []\n",
    "for i in range(8):\n",
    "    question = answers['Question'][i]\n",
    "    topic = answers['Primary Topic'][i]\n",
    "    if topic == 'Inventory':\n",
    "        ans = inv_index.query(str(question))\n",
    "    elif topic== 'Materiality':\n",
    "        ans = mat_index.query(str(question))\n",
    "    output = ans.response\n",
    "    final_ans.append(output)\n",
    "    actual_ans = answers['Answer'][i]\n",
    "    # output scoring\n",
    "    EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "    sim_score_ans = calculate_semantic_similarity(output, actual_ans)\n",
    "    Sbert_score_ans.append(sim_score_ans[1])\n",
    "    NLP_score_ans.append(sim_score_ans[2])\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f885247",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Final answer'] = final_ans\n",
    "answers['EM_Score_ans'] = EM_score_ans\n",
    "answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "answers['NLP_score_ans'] = NLP_score_ans\n",
    "\n",
    "answers.to_csv('ques_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1 = index.query('When is it impracticable to attend an inventory count?')\n",
    "ans1.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bfc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the source nodes\n",
    "ans1.source_nodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3810088f",
   "metadata": {},
   "source": [
    "## Context Generation + OpenAI for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AS, FAQ + openai\n",
    "# list of files to search\n",
    "from dpp_helpline_qa.modelling.question_answer import load_model_qa, answer_question\n",
    "from dpp_helpline_qa.modelling.semantic_search import load_model_ss, context_ranking\n",
    "from dpp_helpline_qa.preprocessing.preprocessing import process_docs\n",
    "\n",
    "files = [\n",
    "    os.path.join(\"data\", \"inventory\", \"FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"data\", \"inventory\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"data\", \"materiality\", \"Audit FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"data\", \"materiality\", \"FAQs 2\" + \".pdf\"),\n",
    "    os.path.join(\"data\", \"materiality\", \"Audit Standard\" + \".pdf\")\n",
    "]\n",
    "\n",
    "# load the model and tokenizer for semantic search\n",
    "model_ss, tokenizer_ss = load_model_ss()\n",
    "max_length = 512\n",
    "\n",
    "# load and pre-process the documents to prepare for searching\n",
    "import time\n",
    "st = time.time()\n",
    "para_dfs = process_docs(files, model_ss, tokenizer_ss, max_length) #'Cosine',  'IVF'\n",
    "para_dfs[0].head()\n",
    "time.time() -st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83807665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the pre-processed files for searching\n",
    "op_files = glob.glob('../output/*/*.csv')\n",
    "op_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "import time\n",
    "import openai\n",
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"model\": \"text-davinci-003\",\n",
    "    }   \n",
    "\n",
    "st = time.time()\n",
    "header = 'Answer the below question based on the given context:\\n'\n",
    "answers = pd.read_excel('LLM_QA.xlsx')\n",
    "context = []\n",
    "final_ans = []\n",
    "EM_score_ans = []\n",
    "Sbert_score_ans = []\n",
    "NLP_score_ans = []\n",
    "EM_score_context = []\n",
    "Sbert_score_context = []\n",
    "NLP_score_context = []\n",
    "for i in range(8):\n",
    "    question = answers['Question'][i]\n",
    "    topic = answers['Primary Topic'][i]\n",
    "    op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "    context_df = context_ranking(question, op_files, model_ss, tokenizer_ss)\n",
    "    main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "    context.append(main_context)\n",
    "    prompt = header + main_context + \"\\n\\n Q: \" + question + \"\\n A:\" \n",
    "    response = openai.Completion.create(\n",
    "                    prompt=prompt,\n",
    "                    **COMPLETIONS_API_PARAMS\n",
    "                )\n",
    "    output = response[\"choices\"][0][\"text\"]\n",
    "    final_ans.append(output)\n",
    "    actual_ans = answers['Answer'][i]\n",
    "    # output scoring\n",
    "    EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "    sim_score_ans = calculate_semantic_similarity(output, actual_ans)\n",
    "    Sbert_score_ans.append(sim_score_ans[1])\n",
    "    NLP_score_ans.append(sim_score_ans[2])\n",
    "    # context scoring\n",
    "    EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "    sim_score_cnxt = calculate_semantic_similarity(main_context, actual_ans)\n",
    "    Sbert_score_context.append(sim_score_cnxt[1])\n",
    "    NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38501850",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Extracted context'] = context\n",
    "answers['Final answer'] = final_ans\n",
    "answers['EM_Score_ans'] = EM_score_ans\n",
    "answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "answers['NLP_score_ans'] = NLP_score_ans\n",
    "answers['EM_Score_context'] = EM_score_context\n",
    "answers['Sbert_score_context'] = Sbert_score_context\n",
    "answers['NLP_score_context'] = NLP_score_context\n",
    "answers.to_csv('ques_score.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
