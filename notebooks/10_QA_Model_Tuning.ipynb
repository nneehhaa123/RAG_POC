{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "\n",
    "\n",
    "from dpp_helpline_qa.model_validation.model_validation import cal_em_score, calculate_semantic_similarity\n",
    "from dpp_helpline_qa.modelling.question_answer import load_model_flan\n",
    "from dpp_helpline_qa.modelling.semantic_search import load_model_ss, context_ranking\n",
    "from dpp_helpline_qa.preprocessing.preprocessing import process_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files to search\n",
    "files = [\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 5\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 6\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 7\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"FAQs 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"ISA_Audit Standard-(UK)-230\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"KAEG-I [UK VERSION 2022]_ ISA (UK) 230 Audit documentation\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"UK_AU_AudFAQ_AD\" + \".pdf\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and tokenizer for semantic search\n",
    "model_semantic = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "model_ss, tokenizer_ss = load_model_ss(model_semantic)\n",
    "max_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and pre-process the documents to prepare for searching\n",
    "import time\n",
    "st = time.time()\n",
    "para_dfs = process_docs(files, model_ss, tokenizer_ss, max_length, 'FlatL2') #'Cosine'\n",
    "para_dfs[0].head()\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the pre-processed files for searching\n",
    "op_files = glob.glob('../output/*/*.*')\n",
    "op_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and tokenizer for question and answering\n",
    "\n",
    "model_checkpoint = \"google/flan-t5-xxl\"\n",
    "model_qa, tokenizer_qa = load_model_flan(model_checkpoint, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "def answer_question_flan(\n",
    "    model: transformers.models.t5.modeling_t5.T5ForConditionalGeneration,\n",
    "    tokenizer: transformers.models.t5.tokenization_t5_fast.T5TokenizerFast,\n",
    "    context: str,\n",
    "    question: str,\n",
    "    min_length = 200,\n",
    "    max_length = 500,\n",
    "    use_gpu: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (transformers.models.t5.modeling_t5.T5ForConditionalGeneration): _description_\n",
    "        tokenizer (transformers.models.t5.tokenization_t5_fast.T5TokenizerFast): _description_\n",
    "        context (str): _description_\n",
    "        question (str): _description_\n",
    "        use_GPU (bool): Use of GPU or not\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    prompt = f\"{context}\\nAnswer this question: {question} Do not repeat sentences to satisfy the minimum output length.\"\n",
    "\n",
    "\n",
    "    if use_gpu:\n",
    "        input_ids = tokenizer(\n",
    "            prompt, return_tensors=\"pt\"\n",
    "        ).input_ids.to(\"cuda\")\n",
    "    else:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(input_ids, max_length=max_length, min_length=min_length)[0]\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "import time\n",
    "st = time.time()\n",
    "\n",
    "min_lengths = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n",
    "max_length = 500\n",
    "\n",
    "all_answers_min = []\n",
    "\n",
    "for min_length in min_lengths:\n",
    "\n",
    "    answers = pd.read_excel('LLM_QA.xlsx')\n",
    "    context = []\n",
    "    final_ans = []\n",
    "    EM_score_ans = []\n",
    "    Sbert_score_ans = []\n",
    "    NLP_score_ans = []\n",
    "    EM_score_context = []\n",
    "    Sbert_score_context = []\n",
    "    NLP_score_context = []\n",
    "    model_val = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "    no_ques = answers.shape[0]\n",
    "    for i in range(no_ques):\n",
    "        question = answers['Question'][i]\n",
    "        topic = answers['Primary Topic'][i]\n",
    "        actual_ans = answers['Answer'][i]\n",
    "        op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "        context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FlatL2')\n",
    "        # answer generated from top 5 contexts\n",
    "        main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "        # answer generated from only 1st context\n",
    "        # ans_context = context_df['content'].values[0]\n",
    "        context.append(main_context)\n",
    "        # QA\n",
    "        output = answer_question_flan(model_qa, tokenizer_qa, main_context, question, min_length, max_length, False)\n",
    "        final_ans.append(output)\n",
    "        # output scoring\n",
    "        EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "        sim_score_ans = calculate_semantic_similarity(model_ss, tokenizer_ss, output, actual_ans) #model_val\n",
    "        Sbert_score_ans.append(sim_score_ans[1])\n",
    "        NLP_score_ans.append(sim_score_ans[2])\n",
    "        # context scoring\n",
    "        EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "        sim_score_cnxt = calculate_semantic_similarity(model_ss, tokenizer_ss, main_context, actual_ans) #model_val\n",
    "        Sbert_score_context.append(sim_score_cnxt[1])\n",
    "        NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "    answers['Extracted context'] = context\n",
    "    answers['Final answer'] = final_ans\n",
    "    answers['EM_Score_ans'] = EM_score_ans\n",
    "    answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "    answers['NLP_score_ans'] = NLP_score_ans\n",
    "    answers['EM_Score_context'] = EM_score_context\n",
    "    answers['Sbert_score_context'] = Sbert_score_context\n",
    "    answers['NLP_score_context'] = NLP_score_context\n",
    "    #answers['context_top5'] = context_5 # remove incase only one ans from all top 5 context \n",
    "    answers.to_csv('ques_score.csv', index=False)\n",
    "\n",
    "    print(f\"\\n\\nMin Length: {min_length}\")\n",
    "    for col in answers.columns[9:]:\n",
    "        print(f\"{col}_mean: {round(answers[col].mean(), 2)}\")\n",
    "\n",
    "    all_answers_min.append(answers)\n",
    "\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers_min[10]['Final answer'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, answers in zip(min_lengths, all_answers_min):\n",
    "    answers.to_csv(f\"min_length_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "import time\n",
    "st = time.time()\n",
    "\n",
    "max_lengths = [300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "min_length = 200\n",
    "\n",
    "all_answers_max = []\n",
    "\n",
    "for max_length in max_lengths:\n",
    "\n",
    "    answers = pd.read_excel('LLM_QA.xlsx')\n",
    "    context = []\n",
    "    final_ans = []\n",
    "    EM_score_ans = []\n",
    "    Sbert_score_ans = []\n",
    "    NLP_score_ans = []\n",
    "    EM_score_context = []\n",
    "    Sbert_score_context = []\n",
    "    NLP_score_context = []\n",
    "    model_val = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "    no_ques = answers.shape[0]\n",
    "    for i in range(no_ques):\n",
    "        question = answers['Question'][i]\n",
    "        topic = answers['Primary Topic'][i]\n",
    "        actual_ans = answers['Answer'][i]\n",
    "        op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "        context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FlatL2')\n",
    "        # answer generated from top 5 contexts\n",
    "        main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "        # answer generated from only 1st context\n",
    "        # ans_context = context_df['content'].values[0]\n",
    "        context.append(main_context)\n",
    "        # QA\n",
    "        output = answer_question_flan(model_qa, tokenizer_qa, main_context, question, min_length, max_length, False)\n",
    "        final_ans.append(output)\n",
    "        # output scoring\n",
    "        EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "        sim_score_ans = calculate_semantic_similarity(model_ss, tokenizer_ss, output, actual_ans) #model_val\n",
    "        Sbert_score_ans.append(sim_score_ans[1])\n",
    "        NLP_score_ans.append(sim_score_ans[2])\n",
    "        # context scoring\n",
    "        EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "        sim_score_cnxt = calculate_semantic_similarity(model_ss, tokenizer_ss, main_context, actual_ans) #model_val\n",
    "        Sbert_score_context.append(sim_score_cnxt[1])\n",
    "        NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "    answers['Extracted context'] = context\n",
    "    answers['Final answer'] = final_ans\n",
    "    answers['EM_Score_ans'] = EM_score_ans\n",
    "    answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "    answers['NLP_score_ans'] = NLP_score_ans\n",
    "    answers['EM_Score_context'] = EM_score_context\n",
    "    answers['Sbert_score_context'] = Sbert_score_context\n",
    "    answers['NLP_score_context'] = NLP_score_context\n",
    "    #answers['context_top5'] = context_5 # remove incase only one ans from all top 5 context \n",
    "    answers.to_csv('ques_score.csv', index=False)\n",
    "\n",
    "    print(f\"\\n\\n Max Length: {max_length}\")\n",
    "    for col in answers.columns[9:]:\n",
    "        print(f\"{col}_mean: {round(answers[col].mean(), 2)}\")\n",
    "\n",
    "    all_answers_max.append(answers)\n",
    "\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, answers in zip(max_lengths, all_answers_max):\n",
    "    answers.to_csv(f\"max_length_{i}.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
