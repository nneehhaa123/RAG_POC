{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93655eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "\n",
    "from dpp_helpline_qa.model_validation.model_validation import cal_em_score, calculate_semantic_similarity\n",
    "from dpp_helpline_qa.modelling.question_answer import load_model_flan, answer_question_flan\n",
    "from dpp_helpline_qa.modelling.semantic_search import load_model_ss, context_ranking\n",
    "from dpp_helpline_qa.preprocessing.preprocessing import process_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files to search\n",
    "files = [\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 5\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 6\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 7\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"FAQs 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 4\" + \".pdf\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_checkpoints = [\n",
    "(\"sentence-transformers/all-MiniLM-L6-v2\", 256),\n",
    "(\"sentence-transformers/all-mpnet-base-v2\", 384),\n",
    "(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\", 512),\n",
    "(\"sentence-transformers/all-distilroberta-v1\", 128),\n",
    "(\"sentence-transformers/paraphrase-MiniLM-L6-v2\", 128),\n",
    "(\"sentence-transformers/bert-base-nli-mean-tokens\", 128),\n",
    "(\"sentence-transformers/all-MiniLM-L12-v2\", 256),\n",
    "(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", 128),\n",
    "(\"sentence-transformers/all-MiniLM-L12-v1\", 128),\n",
    "(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", 128),\n",
    "(\"sentence-transformers/paraphrase-mpnet-base-v2\", 512),\n",
    "(\"sentence-transformers/distiluse-base-multilingual-cased-v2\", 128),\n",
    "(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", 512),\n",
    "(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\", 512),\n",
    "]\n",
    "\n",
    "error_checkpoints = []\n",
    "\n",
    "answers_dfs = []\n",
    "\n",
    "EM_Score_ans_mean = []\n",
    "Sbert_score_ans_mean = []\n",
    "NLP_score_ans_mean = []\n",
    "EM_Score_context_mean = []\n",
    "Sbert_score_context_mean = []\n",
    "NLP_score_context_mean = []\n",
    "\n",
    "for ss_checkpoint, max_length in ss_checkpoints:\n",
    "\n",
    "    try:\n",
    "\n",
    "        # load the model and tokenizer for semantic search\n",
    "        model_ss, tokenizer_ss = load_model_ss(model_checkpoint=ss_checkpoint)\n",
    "\n",
    "        # load and pre-process the documents to prepare for searching\n",
    "        para_dfs = process_docs(files, model_ss, tokenizer_ss, max_length)\n",
    "        para_dfs[0].head()\n",
    "\n",
    "        # identify the pre-processed files for searching\n",
    "        op_files = glob.glob('../output/*/*.csv')\n",
    "        op_files\n",
    "\n",
    "        # load the model and tokenizer for question and answering\n",
    "        model_qa, tokenizer_qa = load_model_flan()\n",
    "\n",
    "        # automatic evaluation process\n",
    "        answers = pd.read_excel('LLM_QA.xlsx')\n",
    "        context = []\n",
    "        final_ans = []\n",
    "        EM_score_ans = []\n",
    "        Sbert_score_ans = []\n",
    "        NLP_score_ans = []\n",
    "        EM_score_context = []\n",
    "        Sbert_score_context = []\n",
    "        NLP_score_context = []\n",
    "        for i in range(8):\n",
    "            question = answers['Question'][i]\n",
    "            topic = answers['Primary Topic'][i]\n",
    "            op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "            context_df = context_ranking(question, op_files, model_ss, tokenizer_ss)\n",
    "            main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "            context.append(main_context)\n",
    "            output = answer_question_flan(model_qa, tokenizer_qa, main_context, question)\n",
    "            final_ans.append(output)\n",
    "            actual_ans = answers['Answer'][i]\n",
    "            # output scoring\n",
    "            EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "            sim_score_ans = calculate_semantic_similarity(output, actual_ans)\n",
    "            Sbert_score_ans.append(sim_score_ans[1])\n",
    "            NLP_score_ans.append(sim_score_ans[2])\n",
    "            # context scoring\n",
    "            EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "            sim_score_cnxt = calculate_semantic_similarity(main_context, actual_ans)\n",
    "            Sbert_score_context.append(sim_score_cnxt[1])\n",
    "            NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "        answers['Extracted context'] = context\n",
    "        answers['Final answer'] = final_ans\n",
    "        answers['EM_Score_ans'] = EM_score_ans\n",
    "        answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "        answers['NLP_score_ans'] = NLP_score_ans\n",
    "        answers['EM_Score_context'] = EM_score_context\n",
    "        answers['Sbert_score_context'] = Sbert_score_context\n",
    "        answers['NLP_score_context'] = NLP_score_context\n",
    "        answers.to_csv(f'ques_score_{ss_checkpoint.split(\"/\")[1]}.csv', index=False)\n",
    "\n",
    "        answers_dfs.append(answers)\n",
    "\n",
    "        EM_Score_ans_mean.append(answers[\"EM_Score_ans\"].mean())\n",
    "        Sbert_score_ans_mean.append(answers[\"Sbert_score_ans\"].mean())\n",
    "        NLP_score_ans_mean.append(answers[\"NLP_score_ans\"].mean())\n",
    "        EM_Score_context_mean.append(answers[\"EM_Score_context\"].mean())\n",
    "        Sbert_score_context_mean.append(answers[\"Sbert_score_context\"].mean())\n",
    "        NLP_score_context_mean.append(answers[\"NLP_score_context\"].mean())\n",
    "\n",
    "    except:\n",
    "        print(f\"Error: {ss_checkpoint}\")\n",
    "        error_checkpoints.append(ss_checkpoint)\n",
    "\n",
    "ss_checkpoints_ = [c for c in ss_checkpoints if c not in error_checkpoints]\n",
    "\n",
    "scores = pd.DataFrame({\n",
    "    \"model_checkpoint\": ss_checkpoints_,\n",
    "    \"EM_Score_ans_mean\": EM_Score_ans_mean,\n",
    "    \"Sbert_score_ans_mean\": Sbert_score_ans_mean,\n",
    "    \"NLP_score_ans_mean\": NLP_score_ans_mean,\n",
    "    \"EM_Score_context_mean\": EM_Score_context_mean,\n",
    "    \"Sbert_score_context_mean\": Sbert_score_context_mean,\n",
    "    \"NLP_score_context_mean\": NLP_score_context_mean,\n",
    "})\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(\"Sbert_score_ans_mean\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
