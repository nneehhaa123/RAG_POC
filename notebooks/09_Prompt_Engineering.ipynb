{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "\n",
    "\n",
    "from dpp_helpline_qa.model_validation.model_validation import cal_em_score, calculate_semantic_similarity\n",
    "from dpp_helpline_qa.modelling.question_answer import load_model_flan, answer_question_flan\n",
    "from dpp_helpline_qa.modelling.semantic_search import load_model_ss, context_ranking\n",
    "from dpp_helpline_qa.preprocessing.preprocessing import process_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files to search\n",
    "files = [\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 5\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 6\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Inventory\", \"KAEG part 7\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit FAQs\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"FAQs 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"Audit Standard\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 1\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 2\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 3\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Materiality\", \"KAEG part 4\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"ISA_Audit Standard-(UK)-230\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"KAEG-I [UK VERSION 2022]_ ISA (UK) 230 Audit documentation\" + \".pdf\"),\n",
    "    os.path.join(\"..\", \"data\", \"Documentation\", \"UK_AU_AudFAQ_AD\" + \".pdf\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and tokenizer for semantic search\n",
    "model_semantic = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "model_ss, tokenizer_ss = load_model_ss(model_semantic)\n",
    "max_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and pre-process the documents to prepare for searching\n",
    "import time\n",
    "st = time.time()\n",
    "para_dfs = process_docs(files, model_ss, tokenizer_ss, max_length, 'FlatL2') #'Cosine'\n",
    "para_dfs[0].head()\n",
    "time.time() -st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the pre-processed files for searching\n",
    "op_files = glob.glob('../output/*/*.*')\n",
    "op_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and tokenizer for question and answering\n",
    "model_checkpoint = \"google/flan-t5-xxl\"\n",
    "model_qa, tokenizer_qa = load_model_flan(model_checkpoint, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic evaluation process\n",
    "import time\n",
    "st = time.time()\n",
    "\n",
    "all_answers = []\n",
    "\n",
    "for i_prompt in range(24):\n",
    "\n",
    "    answers = pd.read_excel('LLM_QA.xlsx')\n",
    "    context = []\n",
    "    final_ans = []\n",
    "    EM_score_ans = []\n",
    "    Sbert_score_ans = []\n",
    "    NLP_score_ans = []\n",
    "    EM_score_context = []\n",
    "    Sbert_score_context = []\n",
    "    NLP_score_context = []\n",
    "    model_val = '/dbfs/FileStore/tables/multi-qa-mpnet-base-cos-v1/'\n",
    "    no_ques = answers.shape[0]\n",
    "    for i in range(no_ques):\n",
    "        question = answers['Question'][i]\n",
    "        topic = answers['Primary Topic'][i]\n",
    "        actual_ans = answers['Answer'][i]\n",
    "        op_files = glob.glob('../output/'+topic+'/*.csv')\n",
    "        context_df = context_ranking(question, op_files, model_ss, tokenizer_ss, 'FlatL2')\n",
    "        # answer generated from top 5 contexts\n",
    "        main_context = '\\n'.join(context_df['content'].values[0:5])\n",
    "        # answer generated from only 1st context\n",
    "        context.append(main_context)\n",
    "        prompt_templates = [\n",
    "            f\"Question: {question} Context: {main_context}\",\n",
    "            f\"Context: {main_context} Question: {question}\",\n",
    "            f\"Question: {question} Context: {main_context} Answer:\",\n",
    "            f\"Context: {main_context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
    "            f\"Read this and answer the question\\n\\n{main_context}\\n\\n{question}\",\n",
    "            f\"Read this and answer the question. If the question is unanswerable, say \\\"unanswerable\\\".\\n\\n{main_context}\\n\\n{question}\",\n",
    "            f\"Answer the question based on this context:\\n\\n{main_context}\\n\\n{question}\",\n",
    "            f\"Answer this question: {question} Here is some context: {main_context}\",\n",
    "            f\"Here is some context: {main_context} Answer this question: {question}\",\n",
    "            f\"Question: {question} Context: {main_context} Answer:\",\n",
    "            f\"Context: {main_context} Question: {question} Answer:\",\n",
    "            f\"{question}\\n\\n{main_context}\",\n",
    "            f\"{main_context}\\n\\n{question}\",\n",
    "            f\"Read this and answer the question\\n\\n{question}\\n\\n{main_context}\",\n",
    "            f\"Read this and answer the question\\n\\n{main_context}\\n\\n{question}\",\n",
    "            f\"Answer this question: {question} Here is some context: {main_context} Answer this question: {question}\",\n",
    "            f\"Answer this question: {question} Here is some context: {main_context}\",\n",
    "            f\"Answer the question based on the context below.\\n\\n{question}\\n\\n{main_context}\",\n",
    "            f\"Answer the question based on the context below.\\n\\nQuestion: {question}\\n\\nContext: {main_context}\",\n",
    "            f\"Answer the question based on the context below.\\n\\nQuestion: {question}\\n\\nContext: {main_context}\\n\\nAnswer:\",\n",
    "            f\"Answer the question based on the context below.\\n\\nQuestion: {question}\\n\\nContext: {main_context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
    "            f\"{main_context}\\nAnswer this question: {question}\",\n",
    "            f\"{main_context}\\nAnswer this question: {question} Do not repeat sentences to satisfy the minimum output length.\",\n",
    "            f\"{main_context}\\nAnswer this question: {question} Do not repeat sentences to satisfy the minimum output length, use a series of full stops instead.\",\n",
    "        ]\n",
    "        # QA\n",
    "        output = answer_question_flan(model_qa, tokenizer_qa, prompt_templates[i_prompt], use_gpu)\n",
    "        final_ans.append(output)\n",
    "        # output scoring\n",
    "        EM_score_ans.append(cal_em_score(output, actual_ans))\n",
    "        sim_score_ans = calculate_semantic_similarity(model_ss, tokenizer_ss, output, actual_ans) #model_val\n",
    "        Sbert_score_ans.append(sim_score_ans[1])\n",
    "        NLP_score_ans.append(sim_score_ans[2])\n",
    "        # context scoring\n",
    "        EM_score_context.append(cal_em_score(main_context, actual_ans))\n",
    "        sim_score_cnxt = calculate_semantic_similarity(model_ss, tokenizer_ss, main_context, actual_ans) #model_val\n",
    "        Sbert_score_context.append(sim_score_cnxt[1])\n",
    "        NLP_score_context.append(sim_score_cnxt[2])\n",
    "\n",
    "    answers['Extracted context'] = context\n",
    "    answers['Final answer'] = final_ans\n",
    "    answers['EM_Score_ans'] = EM_score_ans\n",
    "    answers['Sbert_score_ans'] = Sbert_score_ans\n",
    "    answers['NLP_score_ans'] = NLP_score_ans\n",
    "    answers['EM_Score_context'] = EM_score_context\n",
    "    answers['Sbert_score_context'] = Sbert_score_context\n",
    "    answers['NLP_score_context'] = NLP_score_context\n",
    "    #answers['context_top5'] = context_5 # remove incase only one ans from all top 5 context \n",
    "    answers.to_csv('ques_score.csv', index=False)\n",
    "\n",
    "    print(f\"\\n\\nPrompt {i_prompt+1}\")\n",
    "    for col in answers.columns[9:]:\n",
    "        print(f\"{col}_mean: {round(answers[col].mean(), 2)}\")\n",
    "\n",
    "    all_answers.append(answers)\n",
    "\n",
    "time.time() -st"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
