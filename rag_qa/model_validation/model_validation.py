"""Model validation functions"""
from typing import List, Optional

import nltk
import numpy as np
import pandas as pd
import spacy
import transformers
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from sentence_transformers import util

from rag_qa.preprocessing.preprocessing import get_embeddings

nltk.download("averaged_perceptron_tagger")
nltk.download("punkt")
nltk.download("wordnet")
spacy.cli.download("en_core_web_md")


def nltk2wn_tag(nltk_tag: str) -> Optional[str]:
    """_summary_

    Args:
        nltk_tag (str): _description_

    Returns:
        str: _description_
    """
    if nltk_tag.startswith("J"):
        return wordnet.ADJ
    if nltk_tag.startswith("V"):
        return wordnet.VERB
    if nltk_tag.startswith("N"):
        return wordnet.NOUN
    if nltk_tag.startswith("R"):
        return wordnet.ADV
    return None


def text_preprocess(sentence: str) -> List[str]:
    """_summary_

    Args:
        sentence (_type_): _description_

    Returns:
        _type_: _description_
    """
    lemmatizer = WordNetLemmatizer()
    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))
    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)
    res_words = []
    for word, tag in wn_tagged:
        if tag is None:
            res_words.append(word)
        else:
            res_words.append(lemmatizer.lemmatize(word, tag))
    return res_words


def cal_em_score(
    pred_ans: str,
    actual_ans: str,
) -> np.float64:
    """_summary_

    Args:
        pred_ans (str): _description_
        actual_ans (str): _description_

    Returns:
        float: _description_
    """
    pred = text_preprocess(pred_ans)
    actual = text_preprocess(actual_ans)
    intersection = set(pred).intersection(set(actual))
    return np.round(len(intersection) / len(set(actual)), 2)


def calculate_semantic_similarity(
    model: transformers.models.mpnet.modeling_mpnet.MPNetModel,
    tokenizer: transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast,
    predicted_answer: str,
    expected_answer: str,
) -> List[np.float64]:
    """_summary_

    Args:
        predicted_answer (str): _description_
        expected_answer (str): _description_

    Returns:
        float: _description_
    """
    # Calculate similarity between predicted answer and expected answer using spacy
    nlp = spacy.load("en_core_web_md")

    # Calculate similarity between predicted answer and expected answer using sentence transformer
    sbert_predicted = (
        get_embeddings([predicted_answer], model, tokenizer).cpu().detach().numpy()
    )
    sbert_expected = (
        get_embeddings([expected_answer], model, tokenizer).cpu().detach().numpy()
    )
    sbert_similarity_score = float(
        util.pytorch_cos_sim(sbert_predicted, sbert_expected)
    )

    # Calculate similarity between expected answer and sentence of the question using spaCy
    expected_doc = nlp(expected_answer)
    question_doc = nlp(predicted_answer)
    spacy_similarity_score = expected_doc.similarity(question_doc)

    # Compute the weighted average of the three similarity scores
    weighted_average = (0.5 * sbert_similarity_score) + (0.5 * spacy_similarity_score)

    sim_scores = [
        np.round(weighted_average, 2),
        np.round(sbert_similarity_score, 2),
        np.round(spacy_similarity_score, 2),
    ]

    return sim_scores


def answer_scoring(
    answers: pd.DataFrame,
    model_ss: transformers.models.mpnet.modeling_mpnet.MPNetModel,
    tokenizer_ss: transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast,
) -> pd.DataFrame:
    """Semantically compare the context and answers generated by the tool with the ground truth

    Args:
        answers (pd.DataFrame): a dataframe containing the questions, context, answers and ground truth
        model_ss (transformers.models.mpnet.modeling_mpnet.MPNetModel): the model used to embed text for semantic similarity
        tokenizer_ss (transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast): the tokenizer used to embed text
        for semantic similarity

    Returns:
        pd.DataFrame: a dataframe containing the questions, context, answers, ground truth and model performance scores
    """
    # score the answers against the ground truth
    answers["EM_Score_ans"] = answers.apply(
        lambda row: cal_em_score(row["Generated Answer"], row["Answer"]), axis=1
    )
    answers["sim_score_ans"] = answers.apply(
        lambda row: calculate_semantic_similarity(
            model_ss, tokenizer_ss, row["Generated Answer"], row["Answer"]
        ),
        axis=1,
    )
    answers["Sbert_score_ans"] = answers.apply(
        lambda row: row["sim_score_ans"][1], axis=1
    )
    answers["NLP_score_ans"] = answers.apply(
        lambda row: row["sim_score_ans"][2], axis=1
    )

    # score the context against the ground truth
    answers["EM_Score_context"] = answers.apply(
        lambda row: cal_em_score(row["Extracted Context"], row["Answer"]), axis=1
    )
    answers["sim_score_context"] = answers.apply(
        lambda row: calculate_semantic_similarity(
            model_ss, tokenizer_ss, row["Extracted Context"], row["Answer"]
        ),
        axis=1,
    )
    answers["Sbert_score_context"] = answers.apply(
        lambda row: row["sim_score_context"][1], axis=1
    )
    answers["NLP_score_context"] = answers.apply(
        lambda row: row["sim_score_context"][2], axis=1
    )

    answers = answers.drop(columns=["sim_score_ans", "sim_score_context"])

    score_cols = [
        "EM_Score_ans",
        "Sbert_score_ans",
        "NLP_score_ans",
        "EM_Score_context",
        "Sbert_score_context",
        "NLP_score_context",
    ]

    print("\nModel Performance Scores:\n")
    for col in score_cols:
        print(f"{col}_mean: {round(answers[col].mean(), 2)}")
    print("\n")

    return answers
