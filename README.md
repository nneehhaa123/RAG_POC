# RAG QA Tool

## Overview

This a POC developed to retrieve the relevant context from multiple documents based on a given query and then generate answer via LLM models.

## Approach

* **Pre-processing** - The documents are first pre-processed, by extracting the text from each PDF file and breaking it down into meaningful passages of text. This is usually a subheading followed by a paragraph of body text.
* **Index Creation** - Each passage of text is first embedded, using a pre-trained large language model (LLM), before a [FAISS](https://github.com/facebookresearch/faiss) index is created using the embeddings to allow for faster searching.
* **Semantic Search** - The question is also embedded in the same way. This embedding is used to search the index for the most semantically similar passages of text to the question from the collection of documents.
* **Question & Answer** - The five most semantically similar passages of text are then given to another LLM, along with the original question, and it generates an answer. The answer, five passages of text, and document and page references are then provided as the final output.
* **Score answers** - The tool can also score the generated answers by comparing them with answers provided.

## User Instructions

To install requirements, run:
```shell
pip install -r requirements.txt
```

To install rag_qa in editable mode, run:
```shell
pip install -e .
```

To create an index, located at {index_directory}, from a directory containing PDF files, located at {data_directory}, run:
```shell
python rag_qa/create_index.py --indir {data_directory} --outdir {index_directory}
```

To generate an answer to one or more questions, stored in the "Question" column of a CSV file located at {questions_filepath}, using a previously created index, located at {index_directory}, run:
```shell
python rag_qa/answer_generation.py --filepath {questions_filepath} --index_dir {index_directory}
```

To score the answers generated by the tool against a provided set of answers, stored in the "Answer" column of the CSV file used to generate the answers located at {questions_filepath}, run:
```shell
python rag_qa/answer_scoring.py --filepath {questions_filepath}
```

## Dev Instructions

To install requirements, run:
```shell
pip install -r requirements-dev.txt
```

To run unit tests in the repository, you can run:
```shell
make test
```

Before committing any changes, run
```shell
pre-commit install
```
to install the pre-commit hook.
When you try to commit changes, it will run the checks defined in the `.pre-commit-config.yaml` file.
If any of these checks fail, you will not be able to commit your changes.
This should prevent you getting build errors for linting errors on your PR.
You can read more about pre-commit [here](https://pre-commit.com/).
