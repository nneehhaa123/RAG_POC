# RAG QA Tool

## Overview

This a POC developed to retrieve the relevant context from multiple documents based on a given query and then generate answer via LLM models.

## Approach

* **Pre-processing** - The documents are first pre-processed, by extracting the text from each PDF file and breaking it down into meaningful passages of text. This is usually a subheading followed by a paragraph of body text.
* **Index Creation** - Each passage of text is first embedded, using a pre-trained large language model (LLM), before a [FAISS](https://github.com/facebookresearch/faiss) index is created using the embeddings to allow for faster searching.
* **Semantic Search** - The question is also embedded in the same way. This embedding is used to search the index for the most semantically similar passages of text to the question from the collection of documents.
* **Question & Answer** - The five most semantically similar passages of text are then given to another LLM, along with the original question, and it generates an answer. The answer, five passages of text, and document and page references are then provided as the final output.
* **Score answers** - The tool can also score the generated answers by comparing them with answers provided.

## User Instructions

To install requirements, run:
```shell
pip install -r requirements.txt
```

To install rag_qa in editable mode, run:
```shell
pip install -e .
```

To create an index, located at {index_directory}, from a directory containing PDF files, located at {data_directory}, run:
```shell
python rag_qa/create_index.py --indir {data_directory} --outdir {index_directory}
```

To generate an answer to one or more questions, stored in the "Question" column of a CSV file located at {questions_filepath}, using a previously created index, located at {index_directory}, run:
```shell
python rag_qa/answer_generation.py --filepath {questions_filepath} --index_dir {index_directory}
```

To score the answers generated by the tool against a provided set of answers, stored in the "Answer" column of the CSV file used to generate the answers located at {questions_filepath}, run:
```shell
python rag_qa/answer_scoring.py --filepath {questions_filepath}
```

## Dev Instructions

To install requirements, run:
```shell
pip install -r requirements-dev.txt
```

To run unit tests in the repository, you can run:
```shell
make test
```

Before committing any changes, run
```shell
pre-commit install
```
to install the pre-commit hook.
When you try to commit changes, it will run the checks defined in the `.pre-commit-config.yaml` file.
If any of these checks fail, you will not be able to commit your changes.
This should prevent you getting build errors for linting errors on your PR.
You can read more about pre-commit [here](https://pre-commit.com/).

### Adding symbolic links in the .github/linters folder

This project has two copies of the `setup.cfg` and `.python-lint` files. This is because pre-commit looks for
the configuration files at the root of the repository, whereas the continuous integration workflow defined
in `ci.yaml` is looking for these files in the `.github/linters` folder.

To prevent having to maintain these files in two places, we can add symbolic links.
To add a symbolic link in the `.github/linters` folder to the `setup.cfg` at the root of the repository, run:
```bash
cd .github/linters
rm setup.cfg # Remove current setup.cfg file
ln -s ../../setup.cfg # Add symbolic link to setup.cfg at repository root
```

We can do the same for the `.python-lint` file. From the root of the repository, run:
```bash
cd .github/linters
rm .python-lint # Remove current .python-lint file
ln -s ../../.python-lint # Add symbolic link to .python-lint at repository root
```

Now, when we update the setup.cfg or .python-lint at the root of the repository,
it will also update the corresponding file in the .github/linters folder.
